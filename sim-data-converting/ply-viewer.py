import open3d as o3d
import numpy as np
import os


# ---------- Camera Extrinsics & Robot Base Transform ----------
# NOTE: The extrinsics in the JSON are relative to ROBOT BASE (not world origin)
# Camera world pos: [1.1, -0.6, 1.3], Robot base: [0, 0, 0.62]
# Stored translation = camera_world - robot_base = [1.1, -0.6, 0.68]
#
# So the coordinate system has robot base at origin [0, 0, 0]

EXTRINSICS_MATRIX = np.array([
    [0.8320502943378436, 0.5547001962252293, -0.0, -0.5824352060364905],
    [-0.2690691175985251, 0.4036036763977875, 0.8744746321952062, -0.05650451469569012],
    [0.485071250072666, -0.7276068751089988, 0.485071250072666, -1.2999909501947449],
    [0.0, 0.0, 0.0, 1.0]
])

# Robot base position in the BASE-RELATIVE coordinate system (i.e., at origin)
ROBOT_BASE_IN_BASE_COORDS = np.array([0.0, 0.0, 0.0])


def world_to_pointcloud_frame(point_world, extrinsics):
    """
    Transform a point from world/base-relative coordinates to point cloud coordinates.

    The point cloud generated by depth_to_pointcloud() has two coordinate mismatches
    compared to the standard camera frame used by the extrinsics matrix:

    1. X-axis negation: In depth_to_pointcloud(), X is computed as:
           x = -(u - cx) * z / fx
       The negation corrects for OpenGL/PyBullet's image coordinate convention,
       which produces a mirrored point cloud when viewed in Open3D without it.

    2. Z-axis negation: The extrinsics use OpenGL convention where the camera
       looks down the -Z axis (objects in front have negative Z). But the point
       cloud uses z = depth (positive values), so +Z points into the scene.

    Combined, the point cloud is in a left-handed coordinate system relative
    to the standard OpenGL camera frame.

    Args:
        point_world: [x, y, z] in world/base-relative coordinates
        extrinsics: 4x4 camera_T_world matrix (transforms world -> camera frame)

    Returns:
        [x, y, z] in point cloud coordinates
    """
    # Step 1: Transform to camera frame using extrinsics
    point_homogeneous = np.array([*point_world, 1.0])
    point_camera = extrinsics @ point_homogeneous

    # Step 2: Apply X and Z negation to convert to point cloud frame
    point_pointcloud = np.array([
        -point_camera[0],  # X negated
        point_camera[1],   # Y unchanged
        -point_camera[2]   # Z negated
    ])

    return point_pointcloud


def direction_to_pointcloud_frame(direction_world, extrinsics):
    """
    Transform a direction vector from world frame to point cloud frame.

    Unlike points, direction vectors only need rotation (no translation).

    Args:
        direction_world: [x, y, z] direction in world coordinates
        extrinsics: 4x4 camera_T_world matrix

    Returns:
        [x, y, z] direction in point cloud coordinates
    """
    # Extract rotation matrix (3x3 upper-left of extrinsics)
    R = extrinsics[:3, :3]

    # Rotate direction to camera frame
    direction_camera = R @ np.array(direction_world)

    # Apply X and Z negation (same as for points)
    direction_pointcloud = np.array([
        -direction_camera[0],
        direction_camera[1],
        -direction_camera[2]
    ])

    return direction_pointcloud


def create_coordinate_frame_at(position, x_axis, y_axis, z_axis, size=0.15):
    """
    Create a coordinate frame (3 axis lines) at a given position.

    Args:
        position: [x, y, z] origin of the frame
        x_axis, y_axis, z_axis: Direction vectors for each axis
        size: Length of each axis line

    Returns:
        List of LineSet objects (red=X, green=Y, blue=Z)
    """
    geometries = []

    # Normalize and scale axes
    x_end = position + size * x_axis / np.linalg.norm(x_axis)
    y_end = position + size * y_axis / np.linalg.norm(y_axis)
    z_end = position + size * z_axis / np.linalg.norm(z_axis)

    # X axis - Red
    line_x = o3d.geometry.LineSet()
    line_x.points = o3d.utility.Vector3dVector([position, x_end])
    line_x.lines = o3d.utility.Vector2iVector([[0, 1]])
    line_x.colors = o3d.utility.Vector3dVector([[1, 0, 0]])
    geometries.append(line_x)

    # Y axis - Green
    line_y = o3d.geometry.LineSet()
    line_y.points = o3d.utility.Vector3dVector([position, y_end])
    line_y.lines = o3d.utility.Vector2iVector([[0, 1]])
    line_y.colors = o3d.utility.Vector3dVector([[0, 1, 0]])
    geometries.append(line_y)

    # Z axis - Blue
    line_z = o3d.geometry.LineSet()
    line_z.points = o3d.utility.Vector3dVector([position, z_end])
    line_z.lines = o3d.utility.Vector2iVector([[0, 1]])
    line_z.colors = o3d.utility.Vector3dVector([[0, 0, 1]])
    geometries.append(line_z)

    return geometries


def create_line(start, end, color=[1, 0, 0]):
    """Create a line between two points."""
    line_set = o3d.geometry.LineSet()
    line_set.points = o3d.utility.Vector3dVector([start, end])
    line_set.lines = o3d.utility.Vector2iVector([[0, 1]])
    line_set.colors = o3d.utility.Vector3dVector([color])
    return line_set


def create_sphere(center, radius=0.02, color=[1, 0, 0]):
    """Create a sphere at given position."""
    sphere = o3d.geometry.TriangleMesh.create_sphere(radius=radius)
    sphere.translate(center)
    sphere.paint_uniform_color(color)
    return sphere


# ---------- Workspace utils ----------
# def compute_workspace_bounds(pc_xyz, n_std=2):
#     WORK_SPACE = [
#         [0.6389609647247474, 4.405105314033407],  # X
#         [-1.562868614993293, -1.0280730580106126],  # Y
#         [1.2382057599132916, 2.6079001348631374],  # Z
#     ]
#     return WORK_SPACE
#
def compute_workspace_bounds(pc_xyz, n_std=10):
    if pc_xyz.shape[0] == 0:
        return None

    mean = pc_xyz.mean(axis=0)
    std = pc_xyz.std(axis=0)

    workspace = [
        [mean[0] - n_std * std[0], mean[0] + n_std * std[0]],  # X
        [mean[1] - n_std * std[1], mean[1] + n_std * std[1]],  # Y
        [mean[2] - n_std * std[2], mean[2] + n_std * std[2]],  # Z
    ]

    print(f"Workspace bounds (mean ± {n_std}*std): {workspace}")
    return workspace


def crop_workspace(pc_xyz, workspace_bounds):
    mask = (
        (pc_xyz[:, 0] >= workspace_bounds[0][0])
        & (pc_xyz[:, 0] <= workspace_bounds[0][1])
        & (pc_xyz[:, 1] >= workspace_bounds[1][0])
        & (pc_xyz[:, 1] <= workspace_bounds[1][1])
        & (pc_xyz[:, 2] >= workspace_bounds[2][0])
        & (pc_xyz[:, 2] <= workspace_bounds[2][1])
    )
    return mask


# ---------- Load point cloud ----------
PLY_PATH = "/home/varun-edachali/Research/RRC/policy/data/3D-Fusion-Helpers/sim-data-converting/env_pc.ply"
assert os.path.exists(PLY_PATH)

pcd = o3d.io.read_point_cloud(PLY_PATH)
assert not pcd.is_empty()

pc_xyz = np.asarray(pcd.points)
print("Original points:", pc_xyz.shape[0])

# ---------- Compute workspace and crop ----------
workspace = compute_workspace_bounds(pc_xyz)
mask = crop_workspace(pc_xyz, workspace)
cropped_xyz = pc_xyz[mask]
print("Cropped points:", cropped_xyz.shape[0])

# ---------- Create Open3D clouds ----------
# Raw cloud (no color)
pcd_raw_nocolor = o3d.geometry.PointCloud()
pcd_raw_nocolor.points = o3d.utility.Vector3dVector(pc_xyz)
pcd_raw_nocolor.colors = o3d.utility.Vector3dVector([])

# Cropped cloud (no color)
pcd_cropped_nocolor = o3d.geometry.PointCloud()
pcd_cropped_nocolor.points = o3d.utility.Vector3dVector(cropped_xyz)
pcd_cropped_nocolor.colors = o3d.utility.Vector3dVector([])

# Cropped cloud (with original color)
pcd_cropped_color = o3d.geometry.PointCloud()
pcd_cropped_color.points = o3d.utility.Vector3dVector(cropped_xyz)

# Retain original colors for the cropped points
if np.asarray(pcd.colors).shape[0] == pc_xyz.shape[0]:
    original_colors = np.asarray(pcd.colors)
    pcd_cropped_color.colors = o3d.utility.Vector3dVector(original_colors[mask])
else:
    # If original cloud has no color, just leave empty
    pcd_cropped_color.colors = o3d.utility.Vector3dVector([])

# ---------- Create coordinate axes ----------
axes = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0, 0, 0])


# ---------- Create workspace bounding box ----------
def create_workspace_box(bounds, color=[1, 0, 0]):
    """Create a wireframe box from workspace bounds."""
    x_min, x_max = bounds[0]
    y_min, y_max = bounds[1]
    z_min, z_max = bounds[2]

    # 8 corners of the box
    corners = [
        [x_min, y_min, z_min],  # 0
        [x_max, y_min, z_min],  # 1
        [x_max, y_max, z_min],  # 2
        [x_min, y_max, z_min],  # 3
        [x_min, y_min, z_max],  # 4
        [x_max, y_min, z_max],  # 5
        [x_max, y_max, z_max],  # 6
        [x_min, y_max, z_max],  # 7
    ]

    # 12 edges of the box
    lines = [
        [0, 1],
        [1, 2],
        [2, 3],
        [3, 0],  # bottom face
        [4, 5],
        [5, 6],
        [6, 7],
        [7, 4],  # top face
        [0, 4],
        [1, 5],
        [2, 6],
        [3, 7],  # vertical edges
    ]

    line_set = o3d.geometry.LineSet()
    line_set.points = o3d.utility.Vector3dVector(corners)
    line_set.lines = o3d.utility.Vector2iVector(lines)
    line_set.colors = o3d.utility.Vector3dVector([color] * len(lines))

    return line_set


workspace_box = create_workspace_box(workspace, color=[1, 0, 0])  # Red box

# ---------- Robot Base Marker ----------
origin = np.array([0.0, 0.0, 0.0])

# Transform robot base position from world/base-relative coords to point cloud frame
robot_base_pointcloud = world_to_pointcloud_frame(ROBOT_BASE_IN_BASE_COORDS, EXTRINSICS_MATRIX)

# Robot base orientation is (0, 0, 0) in world, so axes are identity
# Transform the world axes to point cloud frame
ROBOT_BASE_ORIENTATION = [0.0, 0.0, 0.0]  # Euler angles (identity rotation)
robot_x_axis = direction_to_pointcloud_frame([1, 0, 0], EXTRINSICS_MATRIX)
robot_y_axis = direction_to_pointcloud_frame([0, 1, 0], EXTRINSICS_MATRIX)
robot_z_axis = direction_to_pointcloud_frame([0, 0, 1], EXTRINSICS_MATRIX)

print("\n" + "=" * 60)
print("ROBOT BASE TRANSFORM")
print("=" * 60)
print(f"Robot base position (world):     {ROBOT_BASE_IN_BASE_COORDS}")
print(f"Robot base position (pcd frame): {robot_base_pointcloud}")
print(f"Robot base orientation (world):  {ROBOT_BASE_ORIENTATION}")
print(f"Robot X-axis (pcd frame):        {robot_x_axis}")
print(f"Robot Y-axis (pcd frame):        {robot_y_axis}")
print(f"Robot Z-axis (pcd frame):        {robot_z_axis}")
print("=" * 60 + "\n")

# Create purple line from camera origin to robot base
PURPLE = [0.8, 0.2, 0.8]
line_robot_base = create_line(origin, robot_base_pointcloud, color=PURPLE)
sphere_robot_base = create_sphere(robot_base_pointcloud, radius=0.03, color=PURPLE)

# Create coordinate frame at robot base (RGB = XYZ)
robot_base_frame = create_coordinate_frame_at(
    robot_base_pointcloud, robot_x_axis, robot_y_axis, robot_z_axis, size=0.3
)

robot_base_markers = [line_robot_base, sphere_robot_base] + robot_base_frame

print("Robot base markers:")
print("  PURPLE line + sphere: Camera origin to robot base")
print("  RGB coordinate frame: Robot base orientation (Red=X, Green=Y, Blue=Z)")

# ---------- Visualizations ----------
print("1️⃣ Showing RAW point cloud (no color) with axes, workspace box, and robot base markers")
o3d.visualization.draw_geometries(
    [pcd_raw_nocolor, axes, workspace_box] + robot_base_markers,
    window_name="Raw Point Cloud + Axes + Workspace + Robot Base",
    width=1024,
    height=768,
)

print("2️⃣ Showing CROPPED workspace point cloud (no color) with axes, workspace box, and robot base markers")
o3d.visualization.draw_geometries(
    [pcd_cropped_nocolor, axes, workspace_box] + robot_base_markers,
    window_name="Cropped Point Cloud (No Color) + Axes + Workspace + Robot Base",
    width=1024,
    height=768,
)

print("3️⃣ Showing CROPPED workspace point cloud (with color) + axes, workspace box, and robot base markers")
o3d.visualization.draw_geometries(
    [pcd_cropped_color, axes, workspace_box] + robot_base_markers,
    window_name="Cropped Point Cloud (With Color) + Axes + Workspace + Robot Base",
    width=1024,
    height=768,
)
